{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1ã€æ¨¡å‹è°ƒç”¨çš„åˆ†ç±»\n",
    "\n",
    "è§’åº¦1ï¼šæŒ‰ç…§æ¨¡å‹åŠŸèƒ½çš„ä¸åŒ\n",
    "\n",
    "éå¯¹è¯æ¨¡å‹ï¼šï¼ˆLLMsï¼Œ Text Modelï¼‰\n",
    "\n",
    "å¯¹è¯æ¨¡å‹ï¼šï¼ˆChat Modelsï¼‰ ï¼ˆæ¨èï¼‰\n",
    "\n",
    "åµŒå…¥æ¨¡å‹ï¼šï¼ˆEmbeddings Modelsï¼‰ ï¼ˆRAGï¼‰\n",
    "\n",
    "è§’åº¦2ï¼šæŒ‰ç…§æ¨¡å‹è°ƒç”¨æ—¶ï¼Œå‚æ•°ä¹¦å†™çš„ä½ç½®çš„ä¸åŒï¼ˆapi-key, base_url, model-nameï¼‰\n",
    "\n",
    "ç¡¬ç¼–ç ï¼šå°†å‚æ•°ä¹¦å†™åœ¨ä»£ç ä¸­\n",
    "\n",
    "ä½¿ç”¨ç¯å¢ƒå˜é‡\n",
    "\n",
    "ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼‰\n",
    "\n",
    "è§’åº¦2ï¼šå…·ä½“apiçš„è°ƒç”¨\n",
    "\n",
    "ä½¿ç”¨LangChain æä¾›çš„APIï¼ˆæ¨èï¼‰\n",
    "\n",
    "ä½¿ç”¨OpenAI å®˜æ–¹æä¾›çš„API\n",
    "\n",
    "ä½¿ç”¨ç¬¬ä¸‰æ–¹æä¾›çš„API\n"
   ],
   "id": "196dbb5902026cac"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2ã€è§’åº¦1ï¼šéå¯¹è¯æ¨¡å‹çš„è°ƒç”¨",
   "id": "e805fcdbb8ba013"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:04:08.634462100Z",
     "start_time": "2025-12-11T15:04:08.613945900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æ²¡æœ‰openaiçš„api-key å’Œ base_urlï¼Œæ— æ³•è¿è¡Œï¼Œæ‰€ä»¥è¿™é‡Œæ³¨é‡Šæ‰ä»…ä¾›å‚è€ƒ\n",
    "# import os\n",
    "# import dotenv\n",
    "# from langchain_openai import OpenAI\n",
    "# dotenv.load_dotenv()\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "#\n",
    "# llm = OpenAI()\n",
    "# str = llm.invoke(\"è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—\")\n",
    "# print(str)"
   ],
   "id": "49dcad33ceff66e2",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2ã€è§’åº¦1ï¼šå¯¹è¯æ¨¡å‹çš„è°ƒç”¨",
   "id": "a54b4156d1f9fea0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:04:18.059580600Z",
     "start_time": "2025-12-11T15:04:08.635464200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"qwen3-max\",\n",
    "    temperature=0,\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"),\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    ")\n",
    "\n",
    "Messages = [\n",
    "    SystemMessage(content=\"æˆ‘æ˜¯äººå·¥æ™ºèƒ½åŠ©æ‰‹ï¼Œæˆ‘å«å°æ™ºã€‚\"),\n",
    "    HumanMessage(content=\"ä½ å¥½ï¼Œæˆ‘æ˜¯å°æ˜ï¼Œå¾ˆé«˜å…´è®¤è¯†ä½ ã€‚\")\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(Messages)\n",
    "\n",
    "print(type(response))\n",
    "print(response.content)"
   ],
   "id": "71a50e4d91105d75",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.messages.ai.AIMessage'>\n",
      "ä½ å¥½ï¼Œå°æ˜ï¼å¾ˆé«˜å…´è®¤è¯†ä½ ï¼ğŸ˜Š  \n",
      "æˆ‘æ˜¯å°æ™ºï¼Œæœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®ä½ çš„å—ï¼Ÿæˆ–è€…ä½ æƒ³èŠç‚¹ä»€ä¹ˆï¼Ÿ\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 2ã€è§’åº¦1ï¼šåµŒå…¥æ¨¡å‹çš„è°ƒç”¨",
   "id": "7fad6137c955459b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:04:18.158241200Z",
     "start_time": "2025-12-11T15:04:18.141241900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# æ²¡æœ‰openaiçš„api-key å’Œ base_urlï¼Œæ— æ³•è¿è¡Œï¼Œæ‰€ä»¥è¿™é‡Œæ³¨é‡Šæ‰ä»…ä¾›å‚è€ƒ\n",
    "# import os\n",
    "# import dotenv\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# dotenv.load_dotenv()\n",
    "#\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "#\n",
    "# embeddings = OpenAIEmbeddings(\n",
    "#     model=\"text-embedding-ada-002\",\n",
    "# )\n",
    "#\n",
    "# res1 = embeddings.embed_query(\"æˆ‘æ˜¯æ–‡æ¡£ä¸­çš„æ•°æ®\")\n",
    "# print(res1)"
   ],
   "id": "23b3d7fc3533c555",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3ã€è§’åº¦2ï¼šå‚æ•°ä½ç½®ä¸åŒçš„ä¸¾ä¾‹",
   "id": "36be9c93862f08bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.1 ç¡¬ç¼–ç çš„æ–¹å¼\n",
    "ä»¥å¯¹è¯æ¨¡å‹ä¸ºä¾‹"
   ],
   "id": "e58e7b324fd0062"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:04:23.281998500Z",
     "start_time": "2025-12-11T15:04:18.158241200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# è°ƒç”¨éå¯¹è¯æ¨¡å‹\n",
    "# llms = OpenAI(......)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# è°ƒç”¨å¯¹è¯æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    model=\"qwen3-max\",\n",
    "    temperature=0,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    "    api_key=\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    ")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(\"ä»€ä¹ˆæ˜¯langchainï¼Ÿ\")\n",
    "\n",
    "# æŸ¥çœ‹å“åº”æ–‡æœ¬\n",
    "print(response.content)"
   ],
   "id": "20cbf3a5f1f9fd8f",
   "outputs": [
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided. For details, see: https://help.aliyun.com/zh/model-studio/error-code#apikey-error', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}, 'request_id': 'b321571f-07de-402e-9951-a9a902487c1a'}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAuthenticationError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 15\u001B[0m\n\u001B[0;32m      7\u001B[0m llm \u001B[38;5;241m=\u001B[39m ChatOpenAI(\n\u001B[0;32m      8\u001B[0m     model\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mqwen3-max\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m      9\u001B[0m     temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m,\n\u001B[0;32m     10\u001B[0m     base_url\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://dashscope.aliyuncs.com/compatible-mode/v1\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     11\u001B[0m     api_key\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     12\u001B[0m )\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# è°ƒç”¨æ¨¡å‹\u001B[39;00m\n\u001B[1;32m---> 15\u001B[0m response \u001B[38;5;241m=\u001B[39m \u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mä»€ä¹ˆæ˜¯langchainï¼Ÿ\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# æŸ¥çœ‹å“åº”æ–‡æœ¬\u001B[39;00m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28mprint\u001B[39m(response\u001B[38;5;241m.\u001B[39mcontent)\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001B[0m, in \u001B[0;36mBaseChatModel.invoke\u001B[1;34m(self, input, config, stop, **kwargs)\u001B[0m\n\u001B[0;32m    383\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m    384\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21minvoke\u001B[39m(\n\u001B[0;32m    385\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    390\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m    391\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m BaseMessage:\n\u001B[0;32m    392\u001B[0m     config \u001B[38;5;241m=\u001B[39m ensure_config(config)\n\u001B[0;32m    393\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(\n\u001B[0;32m    394\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mChatGeneration\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m--> 395\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate_prompt(\n\u001B[0;32m    396\u001B[0m             [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_convert_input(\u001B[38;5;28minput\u001B[39m)],\n\u001B[0;32m    397\u001B[0m             stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    398\u001B[0m             callbacks\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    399\u001B[0m             tags\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    400\u001B[0m             metadata\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    401\u001B[0m             run_name\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m),\n\u001B[0;32m    402\u001B[0m             run_id\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m),\n\u001B[0;32m    403\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    404\u001B[0m         )\u001B[38;5;241m.\u001B[39mgenerations[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m    405\u001B[0m     )\u001B[38;5;241m.\u001B[39mmessage\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1025\u001B[0m, in \u001B[0;36mBaseChatModel.generate_prompt\u001B[1;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;129m@override\u001B[39m\n\u001B[0;32m   1017\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[0;32m   1018\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1022\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[0;32m   1023\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[0;32m   1024\u001B[0m     prompt_messages \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_messages() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[1;32m-> 1025\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgenerate(prompt_messages, stop\u001B[38;5;241m=\u001B[39mstop, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:842\u001B[0m, in \u001B[0;36mBaseChatModel.generate\u001B[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[0;32m    839\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(input_messages):\n\u001B[0;32m    840\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    841\u001B[0m         results\u001B[38;5;241m.\u001B[39mappend(\n\u001B[1;32m--> 842\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_with_cache(\n\u001B[0;32m    843\u001B[0m                 m,\n\u001B[0;32m    844\u001B[0m                 stop\u001B[38;5;241m=\u001B[39mstop,\n\u001B[0;32m    845\u001B[0m                 run_manager\u001B[38;5;241m=\u001B[39mrun_managers[i] \u001B[38;5;28;01mif\u001B[39;00m run_managers \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    846\u001B[0m                 \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    847\u001B[0m             )\n\u001B[0;32m    848\u001B[0m         )\n\u001B[0;32m    849\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    850\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m run_managers:\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1091\u001B[0m, in \u001B[0;36mBaseChatModel._generate_with_cache\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m   1089\u001B[0m     result \u001B[38;5;241m=\u001B[39m generate_from_stream(\u001B[38;5;28miter\u001B[39m(chunks))\n\u001B[0;32m   1090\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m-> 1091\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(\n\u001B[0;32m   1092\u001B[0m         messages, stop\u001B[38;5;241m=\u001B[39mstop, run_manager\u001B[38;5;241m=\u001B[39mrun_manager, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs\n\u001B[0;32m   1093\u001B[0m     )\n\u001B[0;32m   1094\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1095\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(messages, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\langchain_openai\\chat_models\\base.py:717\u001B[0m, in \u001B[0;36mBaseChatOpenAI._generate\u001B[1;34m(self, messages, stop, run_manager, **kwargs)\u001B[0m\n\u001B[0;32m    715\u001B[0m     generation_info \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mheaders\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mdict\u001B[39m(raw_response\u001B[38;5;241m.\u001B[39mheaders)}\n\u001B[0;32m    716\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 717\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mcreate(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpayload)\n\u001B[0;32m    718\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_create_chat_result(response, generation_info)\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\openai\\_utils\\_utils.py:287\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    285\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    286\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 287\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:925\u001B[0m, in \u001B[0;36mCompletions.create\u001B[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    882\u001B[0m \u001B[38;5;129m@required_args\u001B[39m([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m], [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmessages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstream\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m    883\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mcreate\u001B[39m(\n\u001B[0;32m    884\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    922\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    923\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ChatCompletion \u001B[38;5;241m|\u001B[39m Stream[ChatCompletionChunk]:\n\u001B[0;32m    924\u001B[0m     validate_response_format(response_format)\n\u001B[1;32m--> 925\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/chat/completions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m            \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmessages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmessages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    931\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43maudio\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43maudio\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    932\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfrequency_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrequency_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    933\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunction_call\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunction_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    934\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mfunctions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunctions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    935\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogit_bias\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogit_bias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    936\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlogprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    937\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodalities\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodalities\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mprediction\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprediction\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    944\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mpresence_penalty\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mpresence_penalty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    945\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mreasoning_effort\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mreasoning_effort\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    946\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    947\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mseed\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mseed\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    948\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mservice_tier\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mservice_tier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    949\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstop\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    950\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstore\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstore\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    951\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    952\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    953\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    954\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    955\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    956\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_logprobs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_logprobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    957\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muser\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43muser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    959\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweb_search_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mweb_search_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m            \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsStreaming\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\n\u001B[0;32m    963\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mcompletion_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mCompletionCreateParamsNonStreaming\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    965\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[43m            \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    967\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    968\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mChatCompletion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    969\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    970\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mChatCompletionChunk\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    971\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\openai\\_base_client.py:1239\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1225\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1226\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1227\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1234\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1235\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1236\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1237\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1238\u001B[0m     )\n\u001B[1;32m-> 1239\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\.conda\\envs\\langchain0.3_study_env\\lib\\site-packages\\openai\\_base_client.py:1034\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1031\u001B[0m             err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1033\u001B[0m         log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1034\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m   1038\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcould not resolve response (should never happen)\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mAuthenticationError\u001B[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided. For details, see: https://help.aliyun.com/zh/model-studio/error-code#apikey-error', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}, 'request_id': 'b321571f-07de-402e-9951-a9a902487c1a'}"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2 ä½¿ç”¨ç¯å¢ƒå˜é‡çš„æ–¹å¼\n",
    "ä»¥å¯¹è¯æ¨¡å‹ä¸ºä¾‹\n",
    "\n",
    "è¯´æ˜ï¼šä½¿ç”¨ç¯å¢ƒå˜é‡çš„æ–¹å¼åœ¨jupyterä¸­æ‰§è¡Œä¸åˆé€‚ï¼Œéœ€è¦åœ¨.pyæ–‡ä»¶ä¸­æ‰§è¡Œ"
   ],
   "id": "3fc51c785fb5369c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# è°ƒç”¨éå¯¹è¯æ¨¡å‹\n",
    "# llms = OpenAI(......)\n",
    "\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# è°ƒç”¨å¯¹è¯æ¨¡å‹\n",
    "# llm = ChatOpenAI(\n",
    "#     model=\"qwen3-max\",\n",
    "#     temperature=0,\n",
    "#     base_url=os.environ[\"DASHSCOPE_BASE_URL\"],\n",
    "#     api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n",
    "# )\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "# response = llm.invoke(\"ä»€ä¹ˆæ˜¯langchainï¼Ÿ\")\n",
    "\n",
    "# æŸ¥çœ‹å“åº”æ–‡æœ¬\n",
    "# print(response.content)"
   ],
   "id": "f85f879f31966546",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3.2 ä½¿ç”¨é…ç½®æ–‡ä»¶çš„æ–¹å¼\n",
    "ä»¥å¯¹è¯æ¨¡å‹ä¸ºä¾‹\n",
    "\n",
    "ä½¿ç”¨.envæ–‡ä»¶ä½œä¸ºé…ç½®æ–‡ä»¶"
   ],
   "id": "eab0ebe71764f3aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:05:22.723005800Z",
     "start_time": "2025-12-11T15:04:54.432714600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# è°ƒç”¨éå¯¹è¯æ¨¡å‹\n",
    "# llms = OpenAI(......)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# è°ƒç”¨å¯¹è¯æ¨¡å‹\n",
    "llm = ChatOpenAI(\n",
    "    # å¿…é¡»è¦è®¾ç½®çš„ä¸‰ä¸ªå‚æ•°\n",
    "    model=\"qwen3-max\", # é»˜è®¤ä½¿ç”¨gpt-3.5-turbo\n",
    "    base_url=os.getenv(\"DASHSCOPE_BASE_URL\"), # å½“æ²¡æœ‰æ˜¾å¼çš„ç”³æ˜base_urlæ—¶ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡ä¸­æŸ¥æ‰¾OPENAI_BASE_URL\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"), # å½“æ²¡æœ‰æ˜¾å¼çš„ç”³æ˜api_keyæ—¶ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡ä¸­æŸ¥æ‰¾OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# è°ƒç”¨æ¨¡å‹\n",
    "response = llm.invoke(\"ä»€ä¹ˆæ˜¯langchainï¼Ÿ\")\n",
    "\n",
    "# æŸ¥çœ‹å“åº”æ–‡æœ¬\n",
    "print(response.content)"
   ],
   "id": "e807b75cd4a02c2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain æ˜¯ä¸€ä¸ªå¼€æºæ¡†æ¶ï¼Œæ—¨åœ¨ç®€åŒ–åŸºäºå¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼Œå¦‚ GPTã€Llamaã€Claude ç­‰ï¼‰æ„å»ºåº”ç”¨ç¨‹åºçš„è¿‡ç¨‹ã€‚å®ƒæä¾›äº†ä¸€å¥—æ¨¡å—åŒ–ã€å¯ç»„åˆçš„å·¥å…·å’ŒæŠ½è±¡ï¼Œå¸®åŠ©å¼€å‘è€…å°†è¯­è¨€æ¨¡å‹ä¸å¤–éƒ¨æ•°æ®æºã€è®¡ç®—é€»è¾‘ã€è®°å¿†æœºåˆ¶ç­‰ç»“åˆèµ·æ¥ï¼Œä»è€Œæ„å»ºæ›´å¼ºå¤§ã€æ›´æ™ºèƒ½çš„åº”ç”¨ï¼Œæ¯”å¦‚ï¼š\n",
      "\n",
      "- èŠå¤©æœºå™¨äººï¼ˆChatbotsï¼‰\n",
      "- é—®ç­”ç³»ç»Ÿï¼ˆQuestion Answering over documentsï¼‰\n",
      "- æ™ºèƒ½ä»£ç†ï¼ˆAgents that use toolsï¼‰\n",
      "- è‡ªåŠ¨åŒ–å·¥ä½œæµï¼ˆAutomated workflows with LLMsï¼‰\n",
      "\n",
      "### LangChain çš„æ ¸å¿ƒç»„ä»¶åŒ…æ‹¬ï¼š\n",
      "\n",
      "1. **Modelsï¼ˆæ¨¡å‹ï¼‰**  \n",
      "   æ”¯æŒå¤šç§å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ OpenAIã€Anthropicã€Hugging Face ç­‰ï¼‰ï¼Œæä¾›ç»Ÿä¸€æ¥å£è°ƒç”¨ã€‚\n",
      "\n",
      "2. **Promptsï¼ˆæç¤ºï¼‰**  \n",
      "   æä¾› PromptTemplateã€FewShotPromptTemplate ç­‰å·¥å…·ï¼Œå¸®åŠ©ç»“æ„åŒ–å’Œä¼˜åŒ–è¾“å…¥æç¤ºï¼ˆpromptsï¼‰ï¼Œæå‡æ¨¡å‹è¾“å‡ºè´¨é‡ã€‚\n",
      "\n",
      "3. **Chainsï¼ˆé“¾ï¼‰**  \n",
      "   å°†å¤šä¸ªç»„ä»¶ï¼ˆå¦‚æç¤º + æ¨¡å‹ + åå¤„ç†ï¼‰ä¸²è”æˆä¸€ä¸ªâ€œé“¾â€ï¼Œå®ç°å¤æ‚é€»è¾‘ã€‚ä¾‹å¦‚ï¼šå…ˆæ£€ç´¢æ–‡æ¡£ï¼Œå†ç”¨ LLM å›ç­”é—®é¢˜ã€‚\n",
      "\n",
      "4. **Memoryï¼ˆè®°å¿†ï¼‰**  \n",
      "   æ”¯æŒåœ¨å¯¹è¯ä¸­ä¿ç•™ä¸Šä¸‹æ–‡ï¼ˆå¦‚èŠå¤©å†å²ï¼‰ï¼Œè®©åº”ç”¨å…·å¤‡çŸ­æœŸæˆ–é•¿æœŸè®°å¿†èƒ½åŠ›ã€‚\n",
      "\n",
      "5. **Indexes & Retrievalï¼ˆç´¢å¼•ä¸æ£€ç´¢ï¼‰**  \n",
      "   æ”¯æŒå°†å¤–éƒ¨æ•°æ®ï¼ˆå¦‚ PDFã€ç½‘é¡µã€æ•°æ®åº“ï¼‰åŠ è½½ã€åˆ†å—ã€åµŒå…¥ï¼ˆembeddingï¼‰å¹¶å»ºç«‹å‘é‡ç´¢å¼•ï¼Œå®ç°â€œåŸºäºç§æœ‰æ•°æ®çš„é—®ç­”â€ï¼ˆRAGï¼šRetrieval-Augmented Generationï¼‰ã€‚\n",
      "\n",
      "6. **Agentsï¼ˆæ™ºèƒ½ä½“ï¼‰**  \n",
      "   å…è®¸ LLM è°ƒç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚æœç´¢å¼•æ“ã€è®¡ç®—å™¨ã€APIï¼‰æ¥å®Œæˆä»»åŠ¡ï¼Œå®ç°åŠ¨æ€å†³ç­–å’Œè¡ŒåŠ¨ã€‚\n",
      "\n",
      "7. **Callbacksï¼ˆå›è°ƒï¼‰**  \n",
      "   æä¾›æ—¥å¿—ã€ç›‘æ§ã€æµå¼è¾“å‡ºç­‰èƒ½åŠ›ï¼Œä¾¿äºè°ƒè¯•å’Œé›†æˆã€‚\n",
      "\n",
      "---\n",
      "\n",
      "### ä¸¾ä¸ªç®€å•ä¾‹å­ï¼ˆä½¿ç”¨ LangChain å®ç°æ–‡æ¡£é—®ç­”ï¼‰ï¼š\n",
      "```python\n",
      "from langchain_community.document_loaders import TextLoader\n",
      "from langchain_text_splitters import CharacterTextSplitter\n",
      "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
      "from langchain_chroma import Chroma\n",
      "from langchain.chains import RetrievalQA\n",
      "\n",
      "# 1. åŠ è½½æ–‡æ¡£\n",
      "loader = TextLoader(\"my_document.txt\")\n",
      "documents = loader.load()\n",
      "\n",
      "# 2. åˆ†å—\n",
      "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
      "texts = text_splitter.split_documents(documents)\n",
      "\n",
      "# 3. åˆ›å»ºå‘é‡æ•°æ®åº“\n",
      "embeddings = OpenAIEmbeddings()\n",
      "db = Chroma.from_documents(texts, embeddings)\n",
      "\n",
      "# 4. æ„å»ºé—®ç­”é“¾\n",
      "llm = ChatOpenAI()\n",
      "qa_chain = RetrievalQA.from_chain_type(llm, retriever=db.as_retriever())\n",
      "\n",
      "# 5. æé—®\n",
      "result = qa_chain.invoke(\"æ–‡æ¡£ä¸­æåˆ°äº†ä»€ä¹ˆå…³é”®æŠ€æœ¯ï¼Ÿ\")\n",
      "print(result[\"result\"])\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### é€‚ç”¨åœºæ™¯\n",
      "- ä¼ä¸šçŸ¥è¯†åº“é—®ç­”ç³»ç»Ÿ\n",
      "- æ™ºèƒ½å®¢æœ\n",
      "- è‡ªåŠ¨æŠ¥å‘Šç”Ÿæˆ\n",
      "- ä»£ç åŠ©æ‰‹\n",
      "- å¤šæ­¥éª¤æ¨ç†ä»»åŠ¡\n",
      "\n",
      "### æ³¨æ„äº‹é¡¹\n",
      "- LangChain å‘å±•è¿…é€Ÿï¼ŒAPI å˜åŒ–è¾ƒå¿«ï¼ˆå°¤å…¶ v0.1 â†’ v0.2 æœ‰è¾ƒå¤§é‡æ„ï¼‰ã€‚\n",
      "- å®ƒæœ¬èº«ä¸æä¾›æ¨¡å‹ï¼Œè€Œæ˜¯ä½œä¸ºâ€œèƒ¶æ°´å±‚â€è¿æ¥æ¨¡å‹ä¸åº”ç”¨é€»è¾‘ã€‚\n",
      "- å¯¹äºç®€å•ä»»åŠ¡å¯èƒ½â€œæ€é¸¡ç”¨ç‰›åˆ€â€ï¼Œä½†å¯¹äºå¤æ‚ LLM åº”ç”¨éå¸¸æœ‰ç”¨ã€‚\n",
      "\n",
      "å®˜ç½‘ï¼šhttps://www.langchain.com/  \n",
      "GitHubï¼šhttps://github.com/langchain-ai/langchain\n",
      "\n",
      "å¦‚æœä½ æ­£åœ¨ç”¨å¤§æ¨¡å‹å¼€å‘åº”ç”¨ï¼ŒLangChain èƒ½æ˜¾è‘—æå‡å¼€å‘æ•ˆç‡å’Œç³»ç»Ÿçµæ´»æ€§ã€‚\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4ã€è§’åº¦3ï¼šä½¿ç”¨å„ä¸ªå¹³å°çš„APIè°ƒç”¨å¤§æ¨¡å‹",
   "id": "2dcddaf6f460bfb7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.1 ä½¿ç”¨OpenAI å®˜æ–¹æä¾›çš„APIè°ƒç”¨å¤§æ¨¡å‹",
   "id": "1576c985153ded31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from openai import OpenAI\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key=\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    "#     base_url=\"https://api.openai.com/v1\",\n",
    "# )\n",
    "#\n",
    "# response = client.completions.create(\n",
    "#     model=\"gpt-3.5-turbo-instruct\",\n",
    "#     prompt=\"è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—\",\n",
    "#     max_tokens=100,\n",
    "#     temperature=0.8,\n",
    "# )\n",
    "#\n",
    "# print(response.choices[0].text.strip())"
   ],
   "id": "f3bbca8d952961ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 ç™¾åº¦åƒå¸†å¹³å°",
   "id": "2a837feb6f57437a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# from openai import OpenAI\n",
    "#\n",
    "# client = OpenAI(\n",
    "#     api_key=\"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    "#     base_url=\"https://qianfan.baidubce.com/v2\",\n",
    "#     default_headers={\"appid\": \"your_app_id\"},\n",
    "# )\n",
    "#\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=\"ERNIE-Bot-turbo\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"user\", \"content\": \"è¯·å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—\"},\n",
    "#     ],\n",
    "#     max_tokens=100,\n",
    "#     temperature=0.8,\n",
    "# )"
   ],
   "id": "9439e54d042bf780",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4.3 é˜¿é‡Œäº‘ç™¾ç‚¼å¹³å°\n",
    "æ–¹å¼1ï¼šä½¿ç”¨OpenAIçš„æ–¹å¼"
   ],
   "id": "494e40a357881b48"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:05:27.453120300Z",
     "start_time": "2025-12-11T15:05:22.724011200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key=\"sk-xxx\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\",\n",
    ")\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"qwen3-max\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "    ],\n",
    "    stream=True\n",
    ")\n",
    "for chunk in completion:\n",
    "    print(chunk.choices[0].delta.content, end=\"\", flush=True)"
   ],
   "id": "29886fb5ce8b806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼ˆQwenï¼‰ï¼Œç”±é€šä¹‰å®éªŒå®¤ç ”å‘çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘å¯ä»¥å›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "æ–¹å¼äºŒï¼šä½¿ç”¨dashscope",
   "id": "e4cd1657a89d2e46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T15:05:31.290193500Z",
     "start_time": "2025-12-11T15:05:27.467407900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "   import os\n",
    "from dashscope import Generation\n",
    "import dashscope\n",
    "\n",
    "dashscope.base_http_api_url = 'https://dashscope.aliyuncs.com/api/v1'\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"ä½ æ˜¯è°ï¼Ÿ\"},\n",
    "]\n",
    "response = Generation.call(\n",
    "    # è‹¥æ²¡æœ‰é…ç½®ç¯å¢ƒå˜é‡ï¼Œè¯·ç”¨ç™¾ç‚¼API Keyå°†ä¸‹è¡Œæ›¿æ¢ä¸ºï¼šapi_key = \"sk-xxx\",\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    model=\"qwen3-max\",\n",
    "    messages=messages,\n",
    "    result_format=\"message\",\n",
    ")\n",
    "\n",
    "print(response.output.choices[0].message.content)"
   ],
   "id": "7342853a950120ee",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®ï¼Œé˜¿é‡Œå·´å·´é›†å›¢æ——ä¸‹çš„è¶…å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ã€‚æˆ‘èƒ½å¤Ÿå›ç­”é—®é¢˜ã€åˆ›ä½œæ–‡å­—ï¼Œæ¯”å¦‚å†™æ•…äº‹ã€å†™å…¬æ–‡ã€å†™é‚®ä»¶ã€å†™å‰§æœ¬ã€é€»è¾‘æ¨ç†ã€ç¼–ç¨‹ç­‰ç­‰ï¼Œè¿˜èƒ½è¡¨è¾¾è§‚ç‚¹ï¼Œç©æ¸¸æˆç­‰ã€‚å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦å¸®åŠ©ï¼Œæ¬¢è¿éšæ—¶å‘Šè¯‰æˆ‘ï¼\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9c46da13f3a4ae5d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
